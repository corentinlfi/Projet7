{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308c08c0-8a89-470e-ad2e-56c60d083ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64620e8c-5d0b-489b-b986-747a5a66f052",
   "metadata": {},
   "source": [
    "- Objectifs :\n",
    "    - minimiser les faux négatifs (métrique = recall le plus élevé possible)\n",
    "    - minimiser les faux positifs (métrique = precision le plus élevé possible)\n",
    "- Déséquilibres dans le jeu de données : trop de bons clients (0) par rapport aux mauvais clients (1) --> utiliser l'argument class_weight pour résoudre le pb ds la mise en place des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bda24e-93bf-475e-8062-ef5d34d7cd57",
   "metadata": {},
   "source": [
    "Serveur à héberger sur serveur Databricks, pas local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db977bb1-87e9-4f80-b7e6-1dbd6ebb49a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Experiment: artifact_location='mlflow-artifacts:/445817929057729288', creation_time=1720791165501, experiment_id='445817929057729288', last_update_time=1720791165501, lifecycle_stage='active', name='Projet_7', tags={'mlflow.note.content': 'Classification des clients projet 7',\n",
      " 'project_name': 'Projet_7'}>, <Experiment: artifact_location='mlflow-artifacts:/181467598177086032', creation_time=1720684342295, experiment_id='181467598177086032', last_update_time=1720684342295, lifecycle_stage='active', name='Apple_Models', tags={'mlflow.note.content': 'This is the grocery forecasting project. This '\n",
      "                        'experiment contains the produce models for apples.',\n",
      " 'project_name': 'grocery-forecasting',\n",
      " 'project_quarter': 'Q3-2023',\n",
      " 'store_dept': 'produce',\n",
      " 'team': 'stores-ml'}>, <Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1720645090927, experiment_id='0', last_update_time=1720645090927, lifecycle_stage='active', name='Default', tags={}>]\n"
     ]
    }
   ],
   "source": [
    "# Création de l'expérience MLFlow\n",
    "\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:8080\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "all_experiments = client.search_experiments()\n",
    "print(all_experiments)\n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "#experiment_description = (\"Classification des clients projet 7\")\n",
    "\n",
    "# Tag de l'expérience\n",
    "#experiment_tags = {\n",
    "#    \"project_name\": \"Projet_7\",\n",
    "#    \"mlflow.note.content\": experiment_description,\n",
    "#}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "#projet_7_experiment = client.create_experiment(\n",
    "#    name=\"Projet_7\", tags=experiment_tags\n",
    "#)\n",
    "\n",
    "# Sets the current active experiment to the \"Apple_Models\" experiment and\n",
    "# returns the Experiment metadata\n",
    "projet_7_experiment_current = mlflow.set_experiment(\"Projet_7\")\n",
    "mlflow.set_experiment(\"Projet_7\")\n",
    "# Define an artifact path that the model will be saved to.\n",
    "artifact_path = \"Projet_7\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f9725-88ba-4614-a234-b633ef55524e",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594260a7-7101-4522-bd8d-3c219296bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la data\n",
    "\n",
    "application_train_preprocessed = pd.read_csv(\"../data/application_train_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e537c3-0fd9-446a-8d6a-26804e5b68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = application_train_preprocessed.drop('TARGET', axis=1)\n",
    "y = application_train_preprocessed['TARGET']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d1c8b-ca64-442c-a69c-f5be2abbd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    " # Extraction du nom des variables\n",
    "features = list(X.columns)\n",
    "\n",
    "# Imputation valeurs manquantes par la médiane\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Normalisation avec MinMaxScaler, plus adapté que Standard Scaler\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=features)\n",
    "X_test = pd.DataFrame(X_test, columns=features)\n",
    "\n",
    "X_train.columns = X_train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)\n",
    "X_test.columns = X_test.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)\n",
    "\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Testing data shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c04be-080f-4e8e-a085-cedbbdee8f67",
   "metadata": {},
   "source": [
    "## Mise en place des fonctions modèle et résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19910c-15d9-4c37-b04b-d94536e45451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "\n",
    "# Mise en place de fbeta avec beta=2 comme fonction coût métier\n",
    "def fbeta_score_beta2(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# Modèle\n",
    "def entrainementModeleAvecCV(modele, arguments_modele, param_grid):\n",
    "    modele = modele(**arguments_modele) # argument class_weight pour gérer déséquilibre des données\n",
    "\n",
    "    \"\"\"CROSS VALIDATION\"\"\"\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    scorer = make_scorer(fbeta_score_beta2)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=modele,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=scorer,\n",
    "                               cv=cv,\n",
    "                               verbose=1,\n",
    "                               n_jobs=8)\n",
    "\n",
    "    # Entraînement de la cross validation\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Meilleurs paramètres trouvés :\", grid_search.best_params_)\n",
    "    print(\"Meilleur score F1-bêta :\", grid_search.best_score_)\n",
    "\n",
    "    meilleurModele = grid_search.best_estimator_\n",
    "    \n",
    "    \"\"\"PREDICTIONS\"\"\"\n",
    "    model_pred_proba = meilleurModele.predict_proba(X_test)[:, 1]\n",
    "    model_pred = meilleurModele.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        \"modele\":modele,\n",
    "        \"hyperparametres\": meilleurModele,\n",
    "        \"model_pred_proba\": model_pred_proba,\n",
    "        \"model_pred\": model_pred\n",
    "    }\n",
    "    \n",
    "# Entrainement sur le jeu de train\n",
    "#log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd5b54-69a7-405a-85ae-25a75b26d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, fbeta_score\n",
    "import shap\n",
    "\n",
    "def affichageResultatsModele(model_dict):\n",
    "\n",
    "    model_pred = model_dict[\"model_pred\"]\n",
    "    model_pred_proba = model_dict[\"model_pred_proba\"]\n",
    "    hyperparametres = model_dict[\"hyperparametres\"]\n",
    "    hyperparametres_dict = model_dict[\"hyperparametres\"].get_params()\n",
    "    \n",
    "    \"\"\"MATRICE DE CONFUSION\"\"\"\n",
    "    # Calcul de la matrice de confusion\n",
    "    cm = confusion_matrix(y_test, model_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Création de la matrice de confusion avec les labels\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Classe 0', 'Classe 1'])\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=False, values_format='')\n",
    "    \n",
    "    # Annotations pour chaque type de prédiction\n",
    "    cell_texts = {\n",
    "        (0, 0): f'Vrais négatifs\\n{tn}',  # Vrai Négatif\n",
    "        (0, 1): f'Faux positifs\\n{fp}',  # Faux Positif\n",
    "        (1, 0): f'Faux négatifs\\n{fn}',  # Faux Négatif\n",
    "        (1, 1): f'Vrais positifs\\n{tp}'   # Vrai Positif\n",
    "    }\n",
    "    \n",
    "    for (i, j), text in cell_texts.items():\n",
    "        # Placer le texte au centre de chaque cellule, légèrement en bas\n",
    "        ax.text(j, i, text, ha='center', va='center', color='black', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    \n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.ylabel('Vérité terrain')\n",
    "    plt.title('Matrice de Confusion')\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"METRIQUES\"\"\"\n",
    "    precision = precision_score(y_test, model_pred)\n",
    "    recall = recall_score(y_test, model_pred)\n",
    "    f1 = f1_score(y_test, model_pred)\n",
    "    roc_auc = roc_auc_score(y_test, model_pred_proba)\n",
    "    f1_beta = fbeta_score(y_test, model_pred, beta = 2)\n",
    "    \n",
    "    metrics = {\"precision\":precision, \"recall\":recall, \"f1\":f1, \"f1_beta\":f1_beta, \"roc_auc\":roc_auc}\n",
    "    display(metrics)\n",
    "\n",
    "    \"\"\"FEATURE IMPORTANCE GLOBALE\"\"\"\n",
    "    # Calcul des valeurs SHAP pour l'ensemble de test\n",
    "    explainer = shap.Explainer(hyperparametres, X_train)\n",
    "    shap_values = explainer(X_test)\n",
    "    \n",
    "    # Importance globale des caractéristiques\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "    \"\"\"ENVOI DANS LE MLFLOW\"\"\"\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_params(hyperparametres_dict) #hyperparametres du modèle\n",
    "        mlflow.log_metrics(metrics) #metriques du modele    \n",
    "        mlflow.sklearn.log_model(sk_model=hyperparametres, artifact_path=\"Projet_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d74e50-8f47-4a98-8710-f99018bfbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance locale des caractéristiques pour un échantillon\n",
    "\n",
    "def featureImportanceLocale(model_dict, individu):\n",
    "    hyperparametres = model_dict[\"hyperparametres\"]\n",
    "    shap.initjs()\n",
    "    explainer = shap.Explainer(hyperparametres, X_train)\n",
    "    shap_values = explainer(X_test)\n",
    "    shap.force_plot(shap_values[0].base_values, shap_values[0].values, X_test.iloc[0], feature_names=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536588c6-bf17-4181-abf8-a351a7822979",
   "metadata": {},
   "source": [
    "## Test de différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d850f-4729-4905-a28e-517f1ff25f8e",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e8d26-64a2-4ac0-98e0-47bacf9a3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "arguments_modele={\"class_weight\":\"balanced\"}\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"max_iter\": [100, 200, 300]\n",
    "}\n",
    "\n",
    "LogisticRegression = entrainementModeleAvecCV(LogisticRegression, arguments_modele, param_grid)\n",
    "affichageResultatsModele(LogisticRegression)\n",
    "featureImportanceLocale(LogisticRegression, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef76b9e-efb6-4082-a674-0d968366e20c",
   "metadata": {},
   "source": [
    "#### LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cb708-76b6-4008-b5c6-a3577a9df3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "arguments_modele={\"class_weight\":\"balanced\"}\n",
    "\n",
    "param_grid = {\n",
    "    \"num_leaves\":[31, 63, 127],\n",
    "    \"n_estimators\":[100,200,300],\n",
    "    \"max_depth\":[3,5,7]\n",
    "}\n",
    "\n",
    "LGBMClassifier = entrainementModeleAvecCV(lgb.LGBMClassifier, arguments_modele, param_grid)\n",
    "affichageResultatsModele(LGBMClassifier)\n",
    "featureImportanceLocale(LGBMClassifier, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcb3ea-5dd7-4d64-9b1f-9dfd83af6e37",
   "metadata": {},
   "source": [
    "#### Catboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65877820-368e-41f5-9283-43ceef9e0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "arguments_modele={\"class_weight\":\"balanced\"}\n",
    "\n",
    "param_grid = {\n",
    "    \"num_leaves\":[31, 63, 127],\n",
    "    \"n_estimators\":[100,200,300],\n",
    "    \"max_depth\":[3,5,7]\n",
    "}\n",
    "\n",
    "LGBMClassifier = entrainementModeleAvecCV(lgb.LGBMClassifier, arguments_modele, param_grid)\n",
    "affichageResultatsModele(LGBMClassifier)\n",
    "featureImportanceLocale(LGBMClassifier, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572900a8-72d3-49b5-a33b-0ff26aa511bd",
   "metadata": {},
   "source": [
    "#### Pycaret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
